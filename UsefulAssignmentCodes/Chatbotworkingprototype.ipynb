{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
      "hi\n",
      "ROBO: I am glad! You are talking to me\n",
      "what gravitas\n",
      "ROBO: its the technical fest of vit vellore.\n",
      "when riviera\n",
      "ROBO: it generally happens in last week of ferbuary.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "f=open('chatbotwiki.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw=raw.lower()# converts to lowercase\n",
    "#nltk.download('punkt') # first-time use only\n",
    "#nltk.download('wordnet') # first-time use only\n",
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize)# stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    #print(vals)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you. Please contact the management\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx+1]\n",
    "        return robo_response\n",
    "\n",
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "f=open('chatbotwiki.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw=raw.lower()# converts to lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
    "sent_tokens[:2]\n",
    "['a chatbot (also known as a talkbot, chatterbot, bot, im bot, interactive agent, or artificial conversational entity) is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods.',\n",
    " 'such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the turing test.']\n",
    "word_tokens[:2]\n",
    "['a', 'chatbot', '(', 'also', 'known']\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*nods*\n"
     ]
    }
   ],
   "source": [
    "s=greeting('hello Jarvis')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'is']\n"
     ]
    }
   ],
   "source": [
    "s=LemTokens(word_tokens[:2])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LemTokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9bf3c16564dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLemTokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LemTokens' is not defined"
     ]
    }
   ],
   "source": [
    "s=LemTokens(word_tokens[:2])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    print(sent_tokens)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    print(TfidfVec)\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    print(tfidf)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    print(vals)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    print(idx)\n",
    "    flat = vals.flatten()\n",
    "    print(flat)\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    print(req_tfidf)\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nhow is vit?', 'it is a good place for education\\n\\nwhich building you prefer the most ?', 'i prefer sjt building.', 'which block do u live in?', 'i live in a-block.', 'where is scope dean office?', 'it is in sjt 8th floor.', 'where is sense dean office?', 'it is ground floor of tt.', 'where is tt gallary 1?', 'it is ground floor of tt.', 'what is the procedure for fee payment?', 'please contact financial office in main building.', 'where is anna auditorium?', 'it is near main building.', 'when does summer vacation starts in general?', 'it generally starts from 1st may to mid july.', 'how many blocks are there in boys hostel?', 'there are 15 blocks in boys hostel.', 'whats the in-time for boys hostel?', 'the general in-time for boys hostel is 9pm.', 'what is gravitas?', 'its the technical fest of vit vellore.', 'what is riviera?', 'it is the cultural fest of vit vellore.', 'when does gravitas happen?', 'it generally happens in 2nd week of october.', 'when does rivera happen?', 'it generally happens in last week of ferbuary.', 'What is gravitas?', 'What is gravitas?', 'What is gravitas?']\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function LemNormalize at 0x000001A2FDF426A8>,\n",
      "        use_idf=True, vocabulary=None)\n",
      "  (0, 55)\t1.0\n",
      "  (1, 25)\t0.47812645474750654\n",
      "  (1, 40)\t0.47812645474750654\n",
      "  (1, 16)\t0.47812645474750654\n",
      "  (1, 11)\t0.3629380962242562\n",
      "  (1, 41)\t0.42715479409794077\n",
      "  (2, 11)\t0.5150019436669365\n",
      "  (2, 41)\t0.6061241613808501\n",
      "  (2, 47)\t0.6061241613808501\n",
      "  (3, 9)\t0.5206541401738592\n",
      "  (3, 52)\t0.6366879700309754\n",
      "  (3, 33)\t0.5688125307495037\n",
      "  (4, 33)\t0.6662380624821364\n",
      "  (4, 6)\t0.7457391260354046\n",
      "  (5, 45)\t0.6366879700309754\n",
      "  (5, 14)\t0.5688125307495037\n",
      "  (5, 38)\t0.5206541401738592\n",
      "  (6, 47)\t0.5688125307495037\n",
      "  (6, 4)\t0.6366879700309754\n",
      "  (6, 21)\t0.5206541401738592\n",
      "  (7, 14)\t0.5688125307495037\n",
      "  (7, 38)\t0.5206541401738592\n",
      "  (7, 46)\t0.6366879700309754\n",
      "  (8, 21)\t0.5595814627371933\n",
      "  (8, 27)\t0.611340472340746\n",
      "  :\t:\n",
      "  (22, 19)\t0.49442378747719146\n",
      "  (22, 54)\t0.49442378747719146\n",
      "  (23, 44)\t1.0\n",
      "  (24, 55)\t0.4525635038511582\n",
      "  (24, 19)\t0.49442378747719146\n",
      "  (24, 54)\t0.49442378747719146\n",
      "  (24, 13)\t0.5534225435735166\n",
      "  (25, 15)\t0.5822411430337012\n",
      "  (25, 26)\t0.5063368712690283\n",
      "  (25, 28)\t0.6360960809125462\n",
      "  (26, 24)\t0.3959697133937066\n",
      "  (26, 29)\t0.43259530155742004\n",
      "  (26, 3)\t0.48421616877991364\n",
      "  (26, 56)\t0.43259530155742004\n",
      "  (26, 37)\t0.48421616877991364\n",
      "  (27, 15)\t0.5206541401738592\n",
      "  (27, 28)\t0.5688125307495037\n",
      "  (27, 43)\t0.6366879700309754\n",
      "  (28, 24)\t0.4525635038511582\n",
      "  (28, 29)\t0.49442378747719146\n",
      "  (28, 56)\t0.49442378747719146\n",
      "  (28, 18)\t0.5534225435735166\n",
      "  (29, 26)\t1.0\n",
      "  (30, 26)\t1.0\n",
      "  (31, 26)\t1.0\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.50633687 0.         0.         0.         1.\n",
      "  1.         1.        ]]\n",
      "21\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.         0.50633687 0.         0.         0.         1.\n",
      " 1.         1.        ]\n",
      "1.0\n",
      "what is gravitas?\n"
     ]
    }
   ],
   "source": [
    "s=response('What is gravitas?')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ABHINAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ABHINAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
      "What is Riviera?\n",
      "ROBO: it is the cultural fest of vit vellore.\n",
      "aegrherah\n",
      "ROBO: I am sorry! I don't understand you. Please contact the management\n",
      "\n",
      "ROBO: I am sorry! I don't understand you. Please contact the management\n",
      "When is Riviera?\n",
      "ROBO: it generally happens in last week of ferbuary.\n",
      "Anna auditorium\n",
      "ROBO: it is near main building.\n",
      "SENSE\n",
      "ROBO: it is ground floor of tt.\n",
      "When does Riviera occur?\n",
      "ROBO: it generally happens in last week of ferbuary.\n",
      "Where is TT gallery 1?\n",
      "ROBO: it is ground floor of tt.\n",
      "Where is SCOPE Dean Office?\n",
      "ROBO: it is in sjt 8th floor.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "f=open('chatbotwiki.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw=raw.lower()# converts to lowercase\n",
    "nltk.download('punkt') # first-time use only\n",
    "nltk.download('wordnet') # first-time use only\n",
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize)\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you. Please contact the management\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx+1]\n",
    "        return robo_response\n",
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type 'bye'\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<WordListCorpusReader in '.../corpora/stopwords' (not loaded yet)>\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist=stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'app'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-efec0dcd0b70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflask\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFlask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFlask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'app'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from flask import Flask\n",
    "app=Flask(__name__)\n",
    "import app\n",
    "if __name__=='__main__':\n",
    "    app.debug=True\n",
    "    host=os.environ.get('IP','0.0.0.0')\n",
    "    port=int(os.environ.get('PORT',8080))\n",
    "    app.run(host=host, port=port)\n",
    "    \n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Welcome to our library!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ABHINAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ABHINAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type 'bye'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "f=open('chatbotwiki.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw=raw.lower()# converts to lowercase\n",
    "nltk.download('punkt') # first-time use only\n",
    "nltk.download('wordnet') # first-time use only\n",
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize)\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you. Please contact the management\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx+1]\n",
    "        return robo_response\n",
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type 'bye'\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-873062fc7a4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;31m#corpus=[\"hotel food\",\"hotel food service\",\"water\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ABHINAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ABHINAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
      "anna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO:   (0, 54)\t1.0\n",
      "  (1, 25)\t0.47863425606891713\n",
      "  (1, 40)\t0.47863425606891713\n",
      "  (1, 16)\t0.47863425606891713\n",
      "  (1, 11)\t0.36139638169083893\n",
      "  (1, 41)\t0.42675567020583594\n",
      "  (2, 11)\t0.5137452566493765\n",
      "  (2, 41)\t0.6066571565844526\n",
      "  (2, 46)\t0.6066571565844526\n",
      "  (3, 9)\t0.5195741419589847\n",
      "  (3, 51)\t0.6377426615900692\n",
      "  (3, 32)\t0.5686185088401531\n",
      "  (4, 32)\t0.6654981576413133\n",
      "  (4, 6)\t0.746399492347106\n",
      "  (5, 44)\t0.6377426615900692\n",
      "  (5, 14)\t0.5686185088401531\n",
      "  (5, 38)\t0.5195741419589847\n",
      "  (6, 46)\t0.5686185088401531\n",
      "  (6, 4)\t0.6377426615900692\n",
      "  (6, 21)\t0.5195741419589847\n",
      "  (7, 14)\t0.5686185088401531\n",
      "  (7, 38)\t0.5195741419589847\n",
      "  (7, 45)\t0.6377426615900692\n",
      "  (8, 21)\t0.5592182703928053\n",
      "  (8, 27)\t0.6120047811232838\n",
      "  :\t:\n",
      "  (22, 54)\t0.4516624030975802\n",
      "  (22, 49)\t0.554385524278758\n",
      "  (22, 19)\t0.4942963504307339\n",
      "  (22, 53)\t0.4942963504307339\n",
      "  (23, 43)\t1.0\n",
      "  (24, 54)\t0.45166240309758016\n",
      "  (24, 19)\t0.49429635043073383\n",
      "  (24, 53)\t0.49429635043073383\n",
      "  (24, 13)\t0.5543855242787579\n",
      "  (25, 15)\t0.5426938801366484\n",
      "  (25, 26)\t0.5939205975811199\n",
      "  (25, 36)\t0.5939205975811199\n",
      "  (26, 24)\t0.3950200290024283\n",
      "  (26, 28)\t0.4323073103801343\n",
      "  (26, 3)\t0.48486078180788617\n",
      "  (26, 55)\t0.4323073103801343\n",
      "  (26, 37)\t0.48486078180788617\n",
      "  (27, 15)\t0.5426938801366484\n",
      "  (27, 43)\t0.5939205975811199\n",
      "  (27, 36)\t0.5939205975811199\n",
      "  (28, 24)\t0.45166240309758016\n",
      "  (28, 28)\t0.49429635043073383\n",
      "  (28, 55)\t0.49429635043073383\n",
      "  (28, 18)\t0.5543855242787579\n",
      "  (29, 7)\t1.0\n",
      " dfbeb \n",
      "  (0, 7)\t1.0\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.66549816 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.        ]]\n",
      "it is near main building.\n",
      "riviera\n",
      "ROBO:   (0, 54)\t1.0\n",
      "  (1, 25)\t0.47863425606891713\n",
      "  (1, 40)\t0.47863425606891713\n",
      "  (1, 16)\t0.47863425606891713\n",
      "  (1, 11)\t0.36139638169083893\n",
      "  (1, 41)\t0.42675567020583594\n",
      "  (2, 11)\t0.5137452566493765\n",
      "  (2, 41)\t0.6066571565844526\n",
      "  (2, 46)\t0.6066571565844526\n",
      "  (3, 9)\t0.5195741419589847\n",
      "  (3, 51)\t0.6377426615900692\n",
      "  (3, 32)\t0.5686185088401531\n",
      "  (4, 32)\t0.6654981576413133\n",
      "  (4, 6)\t0.746399492347106\n",
      "  (5, 44)\t0.6377426615900692\n",
      "  (5, 14)\t0.5686185088401531\n",
      "  (5, 38)\t0.5195741419589847\n",
      "  (6, 46)\t0.5686185088401531\n",
      "  (6, 4)\t0.6377426615900692\n",
      "  (6, 21)\t0.5195741419589847\n",
      "  (7, 14)\t0.5686185088401531\n",
      "  (7, 38)\t0.5195741419589847\n",
      "  (7, 45)\t0.6377426615900692\n",
      "  (8, 21)\t0.5592182703928053\n",
      "  (8, 27)\t0.6120047811232838\n",
      "  :\t:\n",
      "  (22, 54)\t0.4516624030975802\n",
      "  (22, 49)\t0.554385524278758\n",
      "  (22, 19)\t0.4942963504307339\n",
      "  (22, 53)\t0.4942963504307339\n",
      "  (23, 43)\t1.0\n",
      "  (24, 54)\t0.45166240309758016\n",
      "  (24, 19)\t0.49429635043073383\n",
      "  (24, 53)\t0.49429635043073383\n",
      "  (24, 13)\t0.5543855242787579\n",
      "  (25, 15)\t0.5426938801366484\n",
      "  (25, 26)\t0.5939205975811199\n",
      "  (25, 36)\t0.5939205975811199\n",
      "  (26, 24)\t0.3950200290024283\n",
      "  (26, 28)\t0.4323073103801343\n",
      "  (26, 3)\t0.48486078180788617\n",
      "  (26, 55)\t0.4323073103801343\n",
      "  (26, 37)\t0.48486078180788617\n",
      "  (27, 15)\t0.5592182703928053\n",
      "  (27, 43)\t0.5592182703928053\n",
      "  (27, 36)\t0.6120047811232838\n",
      "  (28, 24)\t0.45166240309758016\n",
      "  (28, 28)\t0.49429635043073383\n",
      "  (28, 55)\t0.49429635043073383\n",
      "  (28, 18)\t0.5543855242787579\n",
      "  (29, 43)\t1.0\n",
      " dfbeb \n",
      "  (0, 43)\t1.0\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.55921827 0.         1.        ]]\n",
      "it is the cultural fest of vit vellore.\n",
      "gravitas\n",
      "ROBO:   (0, 54)\t1.0\n",
      "  (1, 25)\t0.47863425606891713\n",
      "  (1, 40)\t0.47863425606891713\n",
      "  (1, 16)\t0.47863425606891713\n",
      "  (1, 11)\t0.36139638169083893\n",
      "  (1, 41)\t0.42675567020583594\n",
      "  (2, 11)\t0.5137452566493765\n",
      "  (2, 41)\t0.6066571565844526\n",
      "  (2, 46)\t0.6066571565844526\n",
      "  (3, 9)\t0.5195741419589847\n",
      "  (3, 51)\t0.6377426615900692\n",
      "  (3, 32)\t0.5686185088401531\n",
      "  (4, 32)\t0.6654981576413133\n",
      "  (4, 6)\t0.746399492347106\n",
      "  (5, 44)\t0.6377426615900692\n",
      "  (5, 14)\t0.5686185088401531\n",
      "  (5, 38)\t0.5195741419589847\n",
      "  (6, 46)\t0.5686185088401531\n",
      "  (6, 4)\t0.6377426615900692\n",
      "  (6, 21)\t0.5195741419589847\n",
      "  (7, 14)\t0.5686185088401531\n",
      "  (7, 38)\t0.5195741419589847\n",
      "  (7, 45)\t0.6377426615900692\n",
      "  (8, 21)\t0.5592182703928053\n",
      "  (8, 27)\t0.6120047811232838\n",
      "  :\t:\n",
      "  (22, 54)\t0.4516624030975802\n",
      "  (22, 49)\t0.554385524278758\n",
      "  (22, 19)\t0.4942963504307339\n",
      "  (22, 53)\t0.4942963504307339\n",
      "  (23, 43)\t1.0\n",
      "  (24, 54)\t0.45166240309758016\n",
      "  (24, 19)\t0.49429635043073383\n",
      "  (24, 53)\t0.49429635043073383\n",
      "  (24, 13)\t0.5543855242787579\n",
      "  (25, 15)\t0.5592182703928053\n",
      "  (25, 26)\t0.5592182703928053\n",
      "  (25, 36)\t0.6120047811232838\n",
      "  (26, 24)\t0.3950200290024283\n",
      "  (26, 28)\t0.4323073103801343\n",
      "  (26, 3)\t0.48486078180788617\n",
      "  (26, 55)\t0.4323073103801343\n",
      "  (26, 37)\t0.48486078180788617\n",
      "  (27, 15)\t0.5426938801366484\n",
      "  (27, 43)\t0.5939205975811199\n",
      "  (27, 36)\t0.5939205975811199\n",
      "  (28, 24)\t0.45166240309758016\n",
      "  (28, 28)\t0.49429635043073383\n",
      "  (28, 55)\t0.49429635043073383\n",
      "  (28, 18)\t0.5543855242787579\n",
      "  (29, 26)\t1.0\n",
      " dfbeb \n",
      "  (0, 26)\t1.0\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.         0.\n",
      "  0.         0.55921827 0.         0.         0.         1.        ]]\n",
      "its the technical fest of vit vellore.\n",
      "dean office\n",
      "ROBO:   (0, 54)\t1.0\n",
      "  (1, 25)\t0.47863425606891713\n",
      "  (1, 40)\t0.47863425606891713\n",
      "  (1, 16)\t0.47863425606891713\n",
      "  (1, 11)\t0.36139638169083893\n",
      "  (1, 41)\t0.42675567020583594\n",
      "  (2, 11)\t0.5137452566493765\n",
      "  (2, 41)\t0.6066571565844526\n",
      "  (2, 46)\t0.6066571565844526\n",
      "  (3, 9)\t0.5195741419589847\n",
      "  (3, 51)\t0.6377426615900692\n",
      "  (3, 32)\t0.5686185088401531\n",
      "  (4, 32)\t0.6654981576413133\n",
      "  (4, 6)\t0.746399492347106\n",
      "  (5, 44)\t0.669070542185639\n",
      "  (5, 14)\t0.5450972215021553\n",
      "  (5, 38)\t0.5051867265576614\n",
      "  (6, 46)\t0.5686185088401531\n",
      "  (6, 4)\t0.6377426615900692\n",
      "  (6, 21)\t0.5195741419589847\n",
      "  (7, 14)\t0.5450972215021553\n",
      "  (7, 38)\t0.5051867265576614\n",
      "  (7, 45)\t0.669070542185639\n",
      "  (8, 21)\t0.5592182703928053\n",
      "  (8, 27)\t0.6120047811232838\n",
      "  :\t:\n",
      "  (22, 49)\t0.554385524278758\n",
      "  (22, 19)\t0.4942963504307339\n",
      "  (22, 53)\t0.4942963504307339\n",
      "  (23, 43)\t1.0\n",
      "  (24, 54)\t0.45166240309758016\n",
      "  (24, 19)\t0.49429635043073383\n",
      "  (24, 53)\t0.49429635043073383\n",
      "  (24, 13)\t0.5543855242787579\n",
      "  (25, 15)\t0.5426938801366484\n",
      "  (25, 26)\t0.5939205975811199\n",
      "  (25, 36)\t0.5939205975811199\n",
      "  (26, 24)\t0.3950200290024283\n",
      "  (26, 28)\t0.4323073103801343\n",
      "  (26, 3)\t0.48486078180788617\n",
      "  (26, 55)\t0.4323073103801343\n",
      "  (26, 37)\t0.48486078180788617\n",
      "  (27, 15)\t0.5426938801366484\n",
      "  (27, 43)\t0.5939205975811199\n",
      "  (27, 36)\t0.5939205975811199\n",
      "  (28, 24)\t0.45166240309758016\n",
      "  (28, 28)\t0.49429635043073383\n",
      "  (28, 55)\t0.49429635043073383\n",
      "  (28, 18)\t0.5543855242787579\n",
      "  (29, 14)\t0.7334472901175592\n",
      "  (29, 38)\t0.6797463296106931\n",
      " dfbeb \n",
      "  (0, 14)\t0.7334472901175592\n",
      "  (0, 38)\t0.6797463296106931\n",
      "[[0.         0.         0.         0.         0.         0.7431989\n",
      "  0.         0.7431989  0.         0.         0.         0.\n",
      "  0.25872822 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.        ]]\n",
      "it is in sjt 8th floor.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "f=open('chatbotwiki.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw=raw.lower()# converts to lowercase\n",
    "nltk.download('punkt') # first-time use only\n",
    "nltk.download('wordnet') # first-time use only\n",
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    print(tfidf)\n",
    "    print(' dfbeb ')\n",
    "    print(tfidf[-1])\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    \n",
    "    print(vals)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you. Please contact the management\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx+1]\n",
    "        return robo_response\n",
    "\n",
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
