{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics as sm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=pd.read_csv(\"G_Bot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities_Q</th>\n",
       "      <th>Intent_Q</th>\n",
       "      <th>Type</th>\n",
       "      <th>Entities_A</th>\n",
       "      <th>Intent_A</th>\n",
       "      <th>Result_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is. the definition of. does mean.</td>\n",
       "      <td>info_extraction</td>\n",
       "      <td>what</td>\n",
       "      <td>is the. is a. can be called. defined as. simpl...</td>\n",
       "      <td>Definite_text</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when is. what date. when does occur.</td>\n",
       "      <td>info_extraction</td>\n",
       "      <td>when</td>\n",
       "      <td>occurs on. in the year. falls on. happens on. ...</td>\n",
       "      <td>Specific_text</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is on.</td>\n",
       "      <td>yes_or_no</td>\n",
       "      <td>objective</td>\n",
       "      <td>is on</td>\n",
       "      <td>yes_or_no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you book. you make appointment. meet.</td>\n",
       "      <td>appointment</td>\n",
       "      <td>task_appoint</td>\n",
       "      <td>you book. you make appointment. meet.</td>\n",
       "      <td>Keyword_specific_process</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is. the definition of. does mean.</td>\n",
       "      <td>additional</td>\n",
       "      <td>what</td>\n",
       "      <td>also known as. also called.</td>\n",
       "      <td>additional</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How</td>\n",
       "      <td>info_extraction</td>\n",
       "      <td>how</td>\n",
       "      <td>can be. done. process.</td>\n",
       "      <td>Definite_text</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are. How to. How do.</td>\n",
       "      <td>info_extraction</td>\n",
       "      <td>list</td>\n",
       "      <td>as follows. The following. The steps below. ca...</td>\n",
       "      <td>list</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Entities_Q         Intent_Q          Type  \\\n",
       "0  what is. the definition of. does mean.   info_extraction          what   \n",
       "1     when is. what date. when does occur.  info_extraction          when   \n",
       "2                                   is on.        yes_or_no     objective   \n",
       "3    you book. you make appointment. meet.      appointment  task_appoint   \n",
       "4   what is. the definition of. does mean.       additional          what   \n",
       "5                                      How  info_extraction           how   \n",
       "6                What are. How to. How do.  info_extraction          list   \n",
       "\n",
       "                                          Entities_A  \\\n",
       "0  is the. is a. can be called. defined as. simpl...   \n",
       "1  occurs on. in the year. falls on. happens on. ...   \n",
       "2                                             is on    \n",
       "3              you book. you make appointment. meet.   \n",
       "4                        also known as. also called.   \n",
       "5                             can be. done. process.   \n",
       "6  as follows. The following. The steps below. ca...   \n",
       "\n",
       "                   Intent_A  Result_Tokens  \n",
       "0             Definite_text            NaN  \n",
       "1             Specific_text            NaN  \n",
       "2                 yes_or_no            NaN  \n",
       "3  Keyword_specific_process            NaN  \n",
       "4                additional            NaN  \n",
       "5             Definite_text            NaN  \n",
       "6                      list            NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Result_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Result_Tokens\n",
       "count            0.0\n",
       "mean             NaN\n",
       "std              NaN\n",
       "min              NaN\n",
       "25%              NaN\n",
       "50%              NaN\n",
       "75%              NaN\n",
       "max              NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['additional', 'appointment', 'info_extraction', 'yes_or_no']\n",
      "['how', 'list', 'objective', 'task_appoint', 'what', 'when']\n",
      "['Definite_text', 'Keyword_specific_process', 'Specific_text', 'additional', 'list', 'yes_or_no']\n"
     ]
    }
   ],
   "source": [
    "# Label encoding of species column numerically\n",
    "df=t\n",
    "le = LabelEncoder()\n",
    "le.fit(df['Intent_Q'])\n",
    "print(list(le.classes_))\n",
    "df['IntentQ'] = le.transform(df['Intent_Q'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df['Type'])\n",
    "print(list(le.classes_))\n",
    "df['TypeQ'] = le.transform(df['Type'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df['Intent_A'])\n",
    "print(list(le.classes_))\n",
    "df['IntentA'] = le.transform(df['Intent_A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Entities_Q         Intent_Q          Type  \\\n",
      "0  what is. the definition of. does mean.   info_extraction          what   \n",
      "1     when is. what date. when does occur.  info_extraction          when   \n",
      "2                                   is on.        yes_or_no     objective   \n",
      "3    you book. you make appointment. meet.      appointment  task_appoint   \n",
      "4   what is. the definition of. does mean.       additional          what   \n",
      "5                                      How  info_extraction           how   \n",
      "6                What are. How to. How do.  info_extraction          list   \n",
      "\n",
      "                                          Entities_A  \\\n",
      "0  is the. is a. can be called. defined as. simpl...   \n",
      "1  occurs on. in the year. falls on. happens on. ...   \n",
      "2                                             is on    \n",
      "3              you book. you make appointment. meet.   \n",
      "4                        also known as. also called.   \n",
      "5                             can be. done. process.   \n",
      "6  as follows. The following. The steps below. ca...   \n",
      "\n",
      "                   Intent_A  Result_Tokens  IntentQ  TypeQ  IntentA  \n",
      "0             Definite_text            NaN        2      4        0  \n",
      "1             Specific_text            NaN        2      5        2  \n",
      "2                 yes_or_no            NaN        3      2        5  \n",
      "3  Keyword_specific_process            NaN        1      3        1  \n",
      "4                additional            NaN        0      4        3  \n",
      "5             Definite_text            NaN        2      0        0  \n",
      "6                      list            NaN        2      1        4  \n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ABHINAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt') # one time execution\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is. the definition of. does mean. \n",
      "\n",
      "when is. what date. when does occur.\n",
      "\n",
      "is on.\n",
      "\n",
      "you book. you make appointment. meet.\n",
      "\n",
      "what is. the definition of. does mean.\n",
      "\n",
      "How\n",
      "\n",
      "What are. How to. How do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in df['Entities_Q']:\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentences = []\n",
    "\n",
    "#for i in 0\n",
    "for s in df['Entities_Q']:\n",
    "    sentences.append(sent_tokenize(s))\n",
    "    #sentences.append(sent_tokenize(s))\n",
    "#sentences = [y for x in sentences for y in x] # flatten list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what is.', 'the definition of.', 'does mean.'],\n",
       " ['when is.', 'what date.', 'when does occur.'],\n",
       " ['is on.'],\n",
       " ['you book.', 'you make appointment.', 'meet.'],\n",
       " ['what is.', 'the definition of.', 'does mean.'],\n",
       " ['How'],\n",
       " ['What are.', 'How to.', 'How do.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m=len(sentences)\n",
    "l=[]\n",
    "for i in range (0,m):\n",
    "    n=len(sentences[i])\n",
    "    print()\n",
    "    for s in sentences[i]:\n",
    "        l.append([s,df['TypeQ'][i]])\n",
    "   # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what is.', 4], ['the definition of.', 4], ['does mean.', 4], ['when is.', 5], ['what date.', 5], ['when does occur.', 5], ['is on.', 2], ['you book.', 3], ['you make appointment.', 3], ['meet.', 3], ['what is.', 4], ['the definition of.', 4], ['does mean.', 4], ['How', 0], ['What are.', 1], ['How to.', 1], ['How do.', 1]]\n"
     ]
    }
   ],
   "source": [
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentences2 = []\n",
    "\n",
    "#for i in 0\n",
    "for s in df['Entities_A']:\n",
    "    sentences2.append(sent_tokenize(s))\n",
    "    #sentences.append(sent_tokenize(s))\n",
    "#sentences = [y for x in sentences for y in x] # flatten list\n",
    "m=len(sentences2)\n",
    "o=[]\n",
    "for i in range (0,m):\n",
    "    n=len(sentences2[i])\n",
    "    print()\n",
    "    for s in sentences2[i]:\n",
    "        o.append([s,df['TypeQ'][i]])\n",
    "   # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['is the.', 'is a. can be called.', 'defined as.', 'simply put'],\n",
       " ['occurs on.',\n",
       "  'in the year.',\n",
       "  'falls on.',\n",
       "  'happens on.',\n",
       "  'scheduled on.',\n",
       "  'scheduled for.',\n",
       "  'on.',\n",
       "  'at.'],\n",
       " ['is on'],\n",
       " ['you book.', 'you make appointment.', 'meet.'],\n",
       " ['also known as.', 'also called.'],\n",
       " ['can be.', 'done.', 'process.'],\n",
       " ['as follows.', 'The following.', 'The steps below.', 'can be.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['is the.', 4],\n",
       " ['is a. can be called.', 4],\n",
       " ['defined as.', 4],\n",
       " ['simply put', 4],\n",
       " ['occurs on.', 5],\n",
       " ['in the year.', 5],\n",
       " ['falls on.', 5],\n",
       " ['happens on.', 5],\n",
       " ['scheduled on.', 5],\n",
       " ['scheduled for.', 5],\n",
       " ['on.', 5],\n",
       " ['at.', 5],\n",
       " ['is on', 2],\n",
       " ['you book.', 3],\n",
       " ['you make appointment.', 3],\n",
       " ['meet.', 3],\n",
       " ['also known as.', 4],\n",
       " ['also called.', 4],\n",
       " ['can be.', 0],\n",
       " ['done.', 0],\n",
       " ['process.', 0],\n",
       " ['as follows.', 1],\n",
       " ['The following.', 1],\n",
       " ['The steps below.', 1],\n",
       " ['can be.', 1]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o    #Replace punkt, sentence tokenisers with special symbol instead of period as divider and regex instead of tokeniser. you dont need tokeniser for entity table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "p=len(o)            \n",
    "for i in range (0,p):\n",
    "    if o[i][0] == 'on.':        #example code only\n",
    "        searchkey=o[i][1]\n",
    "        print(searchkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build intent classifier for the types what when etc\n",
    "#, just like you do for sentiment analysis, use johnsnow pipeline\n",
    "#if not, search all,or search document\n",
    "#NO coz REDUNDANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nimport pandas as pd\\nimport nltk\\nnltk.download(\\'punkt\\') # one time execution\\nimport re\\nclean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\\n\\n# make alphabets lowercase\\nclean_sentences = [s.lower() for s in clean_sentences]\\nfrom nltk.corpus import stopwords\\nstop_words = stopwords.words(\\'english\\')\\n# function to remove stopwords\\ndef remove_stopwords(sen):\\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\\n    return sen_new\\n# remove stopwords from the sentences\\nclean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\\n# Extract word vectors\\nword_embeddings = {}\\nf = open(\\'glove.6B.100d.txt\\', encoding=\\'utf-8\\')\\nfor line in f:\\n    values = line.split()\\n    word = values[0]\\n    coefs = np.asarray(values[1:], dtype=\\'float32\\')\\n    word_embeddings[word] = coefs\\nf.close()\\nsentence_vectors = []\\nfor i in clean_sentences:\\n    if len(i) != 0:\\n        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\\n    else:\\n        v = np.zeros((100,))\\n    sentence_vectors.append(v)\\n# similarity matrix\\nsim_mat = np.zeros([len(sentences), len(sentences)])\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nfor i in range(len(sentences)):\\n    for j in range(len(sentences)):\\n        if i != j:\\n            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\\nimport networkx as nx\\n\\nnx_graph = nx.from_numpy_array(sim_mat)\\nscores = nx.pagerank(nx_graph)\\n\\nranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\\n\\n# Extract top 10 sentences as the summary\\nfor i in range(10):\\n    print(ranked_sentences[i][1])\\n    print()\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add TextSummarisation code to rank all the types of tokens individually in lists q0-q5 and a0-a5\n",
    "\"\"\"import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt') # one time execution\n",
    "import re\n",
    "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n",
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "# similarity matrix\n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "# Extract top 10 sentences as the summary\n",
    "for i in range(10):\n",
    "    print(ranked_sentences[i][1])\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intent classification, find easier way than searching strings. search in that type first. If not, then all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add answer token words. rank them tfidf from textsumm code. find similarity \n",
    "#Store in csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The:det\n",
      "quick:amod\n",
      "brown:amod\n",
      "fox:nsubj\n",
      "jumped:ROOT\n",
      "over:prep\n",
      "the:det\n",
      "lazy:amod\n",
      "dog:pobj\n",
      "[[The, 'det'], [quick, 'amod'], [brown, 'amod'], [fox, 'nsubj'], [jumped, 'ROOT'], [over, 'prep'], [the, 'det'], [lazy, 'amod'], [dog, 'pobj']]\n",
      "[fox, dog]\n"
     ]
    }
   ],
   "source": [
    "#Extract entities, pos tag\n",
    "\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "en_nlp=spacy.load('en_core_web_sm') #('en') won't work in windows easily\n",
    "doc=en_nlp(\"The quick brown fox jumped over the lazy dog\")\n",
    "\n",
    "u_sent=next(doc.sents)\n",
    "for word in u_sent:\n",
    "    print(\"%s:%s\" % (word,word.dep_))\n",
    "asent=''\n",
    "apos=''\n",
    "entitylist=[]\n",
    "for word in u_sent:\n",
    "    asent=word\n",
    "    apos=word.dep_\n",
    "    entitylist.append([asent,apos])\n",
    "print(entitylist)\n",
    "el=len(entitylist) \n",
    "poskeys=['nsubj','pobj']\n",
    "\n",
    "a_entities=[]\n",
    "for i in range (0,el):\n",
    "    if entitylist[i][1] in poskeys:    \n",
    "        ewo=entitylist[i][0]\n",
    "        a_entities.append(ewo)\n",
    "print(a_entities)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#understand subject verb object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understand pronoun mapping to nouna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEmmatization, root word handling and wordnet synonym based similariy weightage finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "Write weightage algortihm for words, order, synonyym+relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string \n",
    "f=open('rivierawiki.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw=raw.lower()\n",
    "\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "word_tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    "\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    #sent_tokens.append(user_response)  \n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize)\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you. Please contact the management\"\n",
    "        return robo_response\n",
    "    elif req_tfidf<0.5:\n",
    "        robo_response=robo_response+sent_tokens[idx]+\" This answer might be incorrect, please structure your query in a better way. If not, please contact management \"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]#remember senttoken[idx] is the token cosine similaritywise matched and retrieved\n",
    "        return robo_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. Greet me with a 'hi' to begin. If you want to exit, type 'bye'.You can then also access Management details.\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "respond_tokens=[]\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. Greet me with a 'hi' to begin. If you want to exit, type 'bye'.You can then also access Management details.\")\n",
    "while(flag==True):\n",
    "    user_response = input('')\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                \n",
    "                a=response(user_response)\n",
    "                sent_tokens.remove(user_response)    #understand what it did till here\n",
    "                p=len(l)            \n",
    "                for i in range (0,p):\n",
    "                    if a == l[i][0]:       \n",
    "                        searchkey=l[i][1]\n",
    "                q=len(o)            \n",
    "                for i in range (0,q):\n",
    "                    if o[i][1] == searchkey:       \n",
    "        #                obtained all tokens append\n",
    "                        d=o[i][0]\n",
    "                        respond_tokens.append(d)\n",
    "        #                search in document using same similarity chatbot pipeline function. get tokens from riviera page\n",
    "                        \n",
    "        #                print result here\n",
    "                        \n",
    "                print(response(user_response))#result #Store first, and verify        \n",
    "                \n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string \n",
    "f=open('rivierawiki.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw=raw.lower()\n",
    "\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "word_tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    "\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize)\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you. Please contact the management\"\n",
    "        return robo_response\n",
    "    elif req_tfidf<0.5:\n",
    "        robo_response=robo_response+sent_tokens[idx]+\" This answer might be incorrect, please structure your query in a better way. If not, please contact management \"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]#remember senttoken[idx] is the token cosine similaritywise matched and retrieved\n",
    "        return robo_response\n",
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. Greet me with a 'hi' to begin. If you want to exit, type 'bye'.You can then also access Management details.\")\n",
    "while(flag==True):\n",
    "    user_response = input('')\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                #print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=response(user_response)\n",
    "                sent_tokens.remove(user_response)    #understand what it did till here\n",
    "                p=len(l)            \n",
    "                for i in range (0,p):\n",
    "                    if a == l[i][0]:       \n",
    "                        searchkey=l[i][1]\n",
    "                q=len(o)            \n",
    "                for i in range (0,q):\n",
    "                    if o[i][1] == searchkey:       \n",
    "        #                obtained all tokens append\n",
    "                        d=o[i][0]\n",
    "                        respond_tokens.append(d)\n",
    "        #                search in document using same similarity chatbot pipeline function. get tokens from riviera page\n",
    "                        \n",
    "        #                print result here\n",
    "                        \n",
    "                print(response(user_response))#result #Store first, and verify        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='what is.'\n",
    "p=len(l)            \n",
    "for i in range (0,p):\n",
    "    if a == l[i][0]:       \n",
    "        searchkey=l[i][1]\n",
    "q=len(o)            \n",
    "for i in range (0,q):\n",
    "    if o[i][1] == searchkey:       \n",
    "        #                obtained all tokens append\n",
    "        d=o[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(searchkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For answer words, rank them using tfidf and get similarity value. i think req_tfidf is the weigtage score, but try to write your own algorithm to work with tfidf to give weightage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
