{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics as sm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=pd.read_csv(\"G_Bot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities_Q</th>\n",
       "      <th>Intent_Q</th>\n",
       "      <th>Type</th>\n",
       "      <th>Entities_A</th>\n",
       "      <th>Intent_A</th>\n",
       "      <th>Result_Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is. the definition of. does mean.</td>\n",
       "      <td>info_extraction</td>\n",
       "      <td>what</td>\n",
       "      <td>is the. is a. can be called. defined as. simpl...</td>\n",
       "      <td>Definite_text</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when is. what date. when does occur.</td>\n",
       "      <td>info_extraction</td>\n",
       "      <td>when</td>\n",
       "      <td>occurs on. in the year. falls on. happens on. ...</td>\n",
       "      <td>Specific_text</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is on.</td>\n",
       "      <td>yes_or_no</td>\n",
       "      <td>objective</td>\n",
       "      <td>is on</td>\n",
       "      <td>yes_or_no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you book. you make appointment. meet.</td>\n",
       "      <td>appointment</td>\n",
       "      <td>task_appoint</td>\n",
       "      <td>you book. you make appointment. meet.</td>\n",
       "      <td>Keyword_specific_process</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is. the definition of. does mean.</td>\n",
       "      <td>additional</td>\n",
       "      <td>what</td>\n",
       "      <td>also known as. also called.</td>\n",
       "      <td>additional</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How</td>\n",
       "      <td>info_extraction</td>\n",
       "      <td>how</td>\n",
       "      <td>can be. done. process.</td>\n",
       "      <td>Definite_text</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are. How to. How do.</td>\n",
       "      <td>info_extraction</td>\n",
       "      <td>list</td>\n",
       "      <td>as follows. The following. The steps below. ca...</td>\n",
       "      <td>list</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Entities_Q         Intent_Q          Type  \\\n",
       "0  what is. the definition of. does mean.   info_extraction          what   \n",
       "1     when is. what date. when does occur.  info_extraction          when   \n",
       "2                                   is on.        yes_or_no     objective   \n",
       "3    you book. you make appointment. meet.      appointment  task_appoint   \n",
       "4   what is. the definition of. does mean.       additional          what   \n",
       "5                                      How  info_extraction           how   \n",
       "6                What are. How to. How do.  info_extraction          list   \n",
       "\n",
       "                                          Entities_A  \\\n",
       "0  is the. is a. can be called. defined as. simpl...   \n",
       "1  occurs on. in the year. falls on. happens on. ...   \n",
       "2                                             is on    \n",
       "3              you book. you make appointment. meet.   \n",
       "4                        also known as. also called.   \n",
       "5                             can be. done. process.   \n",
       "6  as follows. The following. The steps below. ca...   \n",
       "\n",
       "                   Intent_A  Result_Tokens  \n",
       "0             Definite_text            NaN  \n",
       "1             Specific_text            NaN  \n",
       "2                 yes_or_no            NaN  \n",
       "3  Keyword_specific_process            NaN  \n",
       "4                additional            NaN  \n",
       "5             Definite_text            NaN  \n",
       "6                      list            NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['additional', 'appointment', 'info_extraction', 'yes_or_no']\n",
      "['how', 'list', 'objective', 'task_appoint', 'what', 'when']\n",
      "['Definite_text', 'Keyword_specific_process', 'Specific_text', 'additional', 'list', 'yes_or_no']\n"
     ]
    }
   ],
   "source": [
    "# Label encoding of column numerically\n",
    "df=t\n",
    "le = LabelEncoder()\n",
    "le.fit(df['Intent_Q'])\n",
    "print(list(le.classes_))\n",
    "df['IntentQ'] = le.transform(df['Intent_Q'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df['Type'])\n",
    "print(list(le.classes_))\n",
    "df['TypeQ'] = le.transform(df['Type'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df['Intent_A'])\n",
    "print(list(le.classes_))\n",
    "df['IntentA'] = le.transform(df['Intent_A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Entities_Q         Intent_Q          Type  \\\n",
      "0  what is. the definition of. does mean.   info_extraction          what   \n",
      "1     when is. what date. when does occur.  info_extraction          when   \n",
      "2                                   is on.        yes_or_no     objective   \n",
      "3    you book. you make appointment. meet.      appointment  task_appoint   \n",
      "4   what is. the definition of. does mean.       additional          what   \n",
      "5                                      How  info_extraction           how   \n",
      "6                What are. How to. How do.  info_extraction          list   \n",
      "\n",
      "                                          Entities_A  \\\n",
      "0  is the. is a. can be called. defined as. simpl...   \n",
      "1  occurs on. in the year. falls on. happens on. ...   \n",
      "2                                             is on    \n",
      "3              you book. you make appointment. meet.   \n",
      "4                        also known as. also called.   \n",
      "5                             can be. done. process.   \n",
      "6  as follows. The following. The steps below. ca...   \n",
      "\n",
      "                   Intent_A  Result_Tokens  IntentQ  TypeQ  IntentA  \n",
      "0             Definite_text            NaN        2      4        0  \n",
      "1             Specific_text            NaN        2      5        2  \n",
      "2                 yes_or_no            NaN        3      2        5  \n",
      "3  Keyword_specific_process            NaN        1      3        1  \n",
      "4                additional            NaN        0      4        3  \n",
      "5             Definite_text            NaN        2      0        0  \n",
      "6                      list            NaN        2      1        4  \n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download('punkt') # one time execution\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is. the definition of. does mean. \n",
      "\n",
      "when is. what date. when does occur.\n",
      "\n",
      "is on.\n",
      "\n",
      "you book. you make appointment. meet.\n",
      "\n",
      "what is. the definition of. does mean.\n",
      "\n",
      "How\n",
      "\n",
      "What are. How to. How do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in df['Entities_Q']:\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentences = []\n",
    "\n",
    "#for i in 0\n",
    "for s in df['Entities_Q']:\n",
    "    sentences.append(sent_tokenize(s))\n",
    "    #sentences.append(sent_tokenize(s))\n",
    "#sentences = [y for x in sentences for y in x] # flatten list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what is.', 'the definition of.', 'does mean.'],\n",
       " ['when is.', 'what date.', 'when does occur.'],\n",
       " ['is on.'],\n",
       " ['you book.', 'you make appointment.', 'meet.'],\n",
       " ['what is.', 'the definition of.', 'does mean.'],\n",
       " ['How'],\n",
       " ['What are.', 'How to.', 'How do.']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m=len(sentences)\n",
    "l=[]\n",
    "for i in range (0,m):\n",
    "    n=len(sentences[i])\n",
    "    print()\n",
    "    for s in sentences[i]:\n",
    "        l.append([s,df['TypeQ'][i]])\n",
    "   # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what is.', 4], ['the definition of.', 4], ['does mean.', 4], ['when is.', 5], ['what date.', 5], ['when does occur.', 5], ['is on.', 2], ['you book.', 3], ['you make appointment.', 3], ['meet.', 3], ['what is.', 4], ['the definition of.', 4], ['does mean.', 4], ['How', 0], ['What are.', 1], ['How to.', 1], ['How do.', 1]]\n"
     ]
    }
   ],
   "source": [
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tokens=[]\n",
    "q2=len(l)\n",
    "for i in range (0,q2):\n",
    "    q_tokens.append(l[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is.',\n",
       " 'the definition of.',\n",
       " 'does mean.',\n",
       " 'when is.',\n",
       " 'what date.',\n",
       " 'when does occur.',\n",
       " 'is on.',\n",
       " 'you book.',\n",
       " 'you make appointment.',\n",
       " 'meet.',\n",
       " 'what is.',\n",
       " 'the definition of.',\n",
       " 'does mean.',\n",
       " 'How',\n",
       " 'What are.',\n",
       " 'How to.',\n",
       " 'How do.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "para=\" \".join(q_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_words=nltk.word_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'is',\n",
       " '.',\n",
       " 'the',\n",
       " 'definition',\n",
       " 'of',\n",
       " '.',\n",
       " 'does',\n",
       " 'mean',\n",
       " '.',\n",
       " 'when',\n",
       " 'is',\n",
       " '.',\n",
       " 'what',\n",
       " 'date',\n",
       " '.',\n",
       " 'when',\n",
       " 'does',\n",
       " 'occur',\n",
       " '.',\n",
       " 'is',\n",
       " 'on',\n",
       " '.',\n",
       " 'you',\n",
       " 'book',\n",
       " '.',\n",
       " 'you',\n",
       " 'make',\n",
       " 'appointment',\n",
       " '.',\n",
       " 'meet',\n",
       " '.',\n",
       " 'what',\n",
       " 'is',\n",
       " '.',\n",
       " 'the',\n",
       " 'definition',\n",
       " 'of',\n",
       " '.',\n",
       " 'does',\n",
       " 'mean',\n",
       " '.',\n",
       " 'How',\n",
       " 'What',\n",
       " 'are',\n",
       " '.',\n",
       " 'How',\n",
       " 'to',\n",
       " '.',\n",
       " 'How',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc=['.',',']\n",
    "q_words=[i for i in q_words if i not in punc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'definition',\n",
       " 'of',\n",
       " 'does',\n",
       " 'mean',\n",
       " 'when',\n",
       " 'is',\n",
       " 'what',\n",
       " 'date',\n",
       " 'when',\n",
       " 'does',\n",
       " 'occur',\n",
       " 'is',\n",
       " 'on',\n",
       " 'you',\n",
       " 'book',\n",
       " 'you',\n",
       " 'make',\n",
       " 'appointment',\n",
       " 'meet',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'definition',\n",
       " 'of',\n",
       " 'does',\n",
       " 'mean',\n",
       " 'How',\n",
       " 'What',\n",
       " 'are',\n",
       " 'How',\n",
       " 'to',\n",
       " 'How',\n",
       " 'do']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1): \n",
    "  \n",
    "    # intilize a null list \n",
    "    unique_list = [] \n",
    "      \n",
    "    # traverse for all elements \n",
    "    for x in list1: \n",
    "        # check if exists in unique_list or not \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x)\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_words=unique(q_words) #coz u need all those words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'definition',\n",
       " 'of',\n",
       " 'does',\n",
       " 'mean',\n",
       " 'when',\n",
       " 'is',\n",
       " 'what',\n",
       " 'date',\n",
       " 'when',\n",
       " 'does',\n",
       " 'occur',\n",
       " 'is',\n",
       " 'on',\n",
       " 'you',\n",
       " 'book',\n",
       " 'you',\n",
       " 'make',\n",
       " 'appointment',\n",
       " 'meet',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'definition',\n",
       " 'of',\n",
       " 'does',\n",
       " 'mean',\n",
       " 'How',\n",
       " 'What',\n",
       " 'are',\n",
       " 'How',\n",
       " 'to',\n",
       " 'How',\n",
       " 'do']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentences2 = []\n",
    "\n",
    "#for i in 0\n",
    "for s in df['Entities_A']:\n",
    "    sentences2.append(sent_tokenize(s))\n",
    "    #sentences.append(sent_tokenize(s))\n",
    "#sentences = [y for x in sentences for y in x] # flatten list\n",
    "m=len(sentences2)\n",
    "o=[]\n",
    "for i in range (0,m):\n",
    "    n=len(sentences2[i])\n",
    "    print()\n",
    "    for s in sentences2[i]:\n",
    "        o.append([s,df['TypeQ'][i]])\n",
    "   # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['is the.', 'is a. can be called.', 'defined as.', 'simply put'],\n",
       " ['occurs on.',\n",
       "  'in the year.',\n",
       "  'falls on.',\n",
       "  'happens on.',\n",
       "  'scheduled on.',\n",
       "  'scheduled for.',\n",
       "  'on.',\n",
       "  'at.'],\n",
       " ['is on'],\n",
       " ['you book.', 'you make appointment.', 'meet.'],\n",
       " ['also known as.', 'also called.'],\n",
       " ['can be.', 'done.', 'process.'],\n",
       " ['as follows.', 'The following.', 'The steps below.', 'can be.']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['is the.', 4],\n",
       " ['is a. can be called.', 4],\n",
       " ['defined as.', 4],\n",
       " ['simply put', 4],\n",
       " ['occurs on.', 5],\n",
       " ['in the year.', 5],\n",
       " ['falls on.', 5],\n",
       " ['happens on.', 5],\n",
       " ['scheduled on.', 5],\n",
       " ['scheduled for.', 5],\n",
       " ['on.', 5],\n",
       " ['at.', 5],\n",
       " ['is on', 2],\n",
       " ['you book.', 3],\n",
       " ['you make appointment.', 3],\n",
       " ['meet.', 3],\n",
       " ['also known as.', 4],\n",
       " ['also called.', 4],\n",
       " ['can be.', 0],\n",
       " ['done.', 0],\n",
       " ['process.', 0],\n",
       " ['as follows.', 1],\n",
       " ['The following.', 1],\n",
       " ['The steps below.', 1],\n",
       " ['can be.', 1]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o    #Replace punkt, sentence tokenisers with special symbol instead of period as divider and regex instead of tokeniser. you dont need tokeniser for entity table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nimport pandas as pd\\nimport nltk\\nnltk.download(\\'punkt\\') # one time execution\\nimport re\\nclean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\\n\\n# make alphabets lowercase\\nclean_sentences = [s.lower() for s in clean_sentences]\\nfrom nltk.corpus import stopwords\\nstop_words = stopwords.words(\\'english\\')\\n# function to remove stopwords\\ndef remove_stopwords(sen):\\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\\n    return sen_new\\n# remove stopwords from the sentences\\nclean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\\n# Extract word vectors\\nword_embeddings = {}\\nf = open(\\'glove.6B.100d.txt\\', encoding=\\'utf-8\\')\\nfor line in f:\\n    values = line.split()\\n    word = values[0]\\n    coefs = np.asarray(values[1:], dtype=\\'float32\\')\\n    word_embeddings[word] = coefs\\nf.close()\\nsentence_vectors = []\\nfor i in clean_sentences:\\n    if len(i) != 0:\\n        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\\n    else:\\n        v = np.zeros((100,))\\n    sentence_vectors.append(v)\\n# similarity matrix\\nsim_mat = np.zeros([len(sentences), len(sentences)])\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nfor i in range(len(sentences)):\\n    for j in range(len(sentences)):\\n        if i != j:\\n            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\\nimport networkx as nx\\n\\nnx_graph = nx.from_numpy_array(sim_mat)\\nscores = nx.pagerank(nx_graph)\\n\\nranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\\n\\n# Extract top 10 sentences as the summary\\nfor i in range(10):\\n    print(ranked_sentences[i][1])\\n    print()\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add TextSummarisation code to rank all the types of tokens individually in lists q0-q5 and a0-a5\n",
    "\"\"\"import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt') # one time execution\n",
    "import re\n",
    "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# make alphabets lowercase\n",
    "clean_sentences = [s.lower() for s in clean_sentences]\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(sen):\n",
    "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "    return sen_new\n",
    "# remove stopwords from the sentences\n",
    "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n",
    "# Extract word vectors\n",
    "word_embeddings = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()\n",
    "sentence_vectors = []\n",
    "for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "        v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "# similarity matrix\n",
    "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        if i != j:\n",
    "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "import networkx as nx\n",
    "\n",
    "nx_graph = nx.from_numpy_array(sim_mat)\n",
    "scores = nx.pagerank(nx_graph)\n",
    "\n",
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "# Extract top 10 sentences as the summary\n",
    "for i in range(10):\n",
    "    print(ranked_sentences[i][1])\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build intent classifier for the types what when etc\n",
    "#, just like you do for sentiment analysis, use johnsnow pipeline\n",
    "#if not, search all,or search document\n",
    "#NO coz REDUNDANT\n",
    "\n",
    "#BIGRAM MODEL\n",
    "\n",
    "#USE CLASSIFIERS DOC, of lapd serious classifer. make your entity table like that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(description):\n",
    "    \"\"\"\n",
    "    Takes LAPD description text, strips out unwanted words and text,\n",
    "    and prepares it for the trainer.\n",
    "    \"\"\"\n",
    "    # first lower case and strip leading/trailing whitespace\n",
    "    description = description.lower().strip()\n",
    "    # kill any stray punctuation\n",
    "    description = description.replace('.', '').replace(',', '')\n",
    "    # make a list of words by splitting on whitespace\n",
    "    words = description.split(' ')\n",
    "    # Make sure each \"word\" is a real string / account for odd whitespace\n",
    "   # words = [stemmer.stem(i) for i in words if i] #ADD WORDNET here to find words, if word in dictionary, or if proper noun, or ignore if you don't want users to use capital letters\n",
    "   # words = [i for i in words if i not in STOPWORDS] #stopword handling\n",
    "    # let's see if adding bigrams improves the accuracy\n",
    "    bigrams = ngrams(words, 2)\n",
    "    bigrams = [\"%s|%s\" % (i[0], i[1]) for i in bigrams]\n",
    "    # bigrams = [i for i in bigrams if STEMMED_BIGRAMS.get(i)]\n",
    "    # set up a dict\n",
    "    out_dict = dict([(i, True) for i in words + bigrams])\n",
    "    # The NLTK trainer expects data in a certain format\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"fc = open('G_Bot.csv', 'rU')\\ndata = list(csv.DictReader(fc))\\n# Make an empty list for our processed data\\nfeatures = []\\n# Loop through all the lines in the CSV\\nfor i in data:\\n    description = i.get('NARRATIVE')\\n    classification = i.get('classification')\\n    feats = tokenize(description)\\n    features.append((feats, classification))\\n\\nfc.close()\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open our sample file and use the CSV module to parse it\n",
    "'''fc = open('G_Bot.csv', 'rU')\n",
    "data = list(csv.DictReader(fc))\n",
    "# Make an empty list for our processed data\n",
    "features = []\n",
    "# Loop through all the lines in the CSV\n",
    "for i in data:\n",
    "    description = i.get('NARRATIVE')\n",
    "    classification = i.get('classification')\n",
    "    feats = tokenize(description)\n",
    "    features.append((feats, classification))\n",
    "\n",
    "fc.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add answer token words. rank them tfidf from textsumm code. find similarity \n",
    "#Store in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TryPOStagging, dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS TAG TO LIST, FIND NSUB, list of words to a_words\n",
    "#Extract entities, pos tag\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "en_nlp=spacy.load('en_core_web_sm') #same as ('en') won't work in windows easily\n",
    "#\n",
    "#Add code here  #Define function to do this for 'useresponse, when user enters question, extract entities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee=[]\n",
    "def entityextract(userstring):  #SPACY USES DIFFERENT Datatype probs, so dont take directly\n",
    "    doc=en_nlp(userstring)\n",
    "    a_entity=[]\n",
    "    u_sent=next(doc.sents)\n",
    "    \n",
    "    for word in u_sent:\n",
    "        print(\"%s:%s\" % (word,word.dep_))\n",
    "    asent=''\n",
    "    apos=''\n",
    "    entitylist=[]\n",
    "    for word in u_sent:\n",
    "        asent=word\n",
    "        apos=word.dep_\n",
    "        entitylist.append([asent,apos])\n",
    "\n",
    "    el=len(entitylist) \n",
    "    poskeys=['nsubj','pobj']\n",
    "\n",
    "    for i in range (0,el):\n",
    "        if entitylist[i][1] in poskeys:\n",
    "            d=str(entitylist[i][0])        #Convert spacy.text to string\n",
    "            \n",
    "            a_entity.append(d)\n",
    "    ee=a_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understand pronoun mapping to nouna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what is.', 'the definition of.', 'does mean.', 'when is.', 'what date.', 'when does occur.', 'is on.', 'you book.', 'you make appointment.', 'meet.', 'what is.', 'the definition of.', 'does mean.', 'How', 'What are.', 'How to.', 'How do.']\n",
      "['what', 'is', 'the', 'definition', 'of', 'does', 'mean', 'when', 'is', 'what', 'date', 'when', 'does', 'occur', 'is', 'on', 'you', 'book', 'you', 'make', 'appointment', 'meet', 'what', 'is', 'the', 'definition', 'of', 'does', 'mean', 'How', 'What', 'are', 'How', 'to', 'How', 'do']\n"
     ]
    }
   ],
   "source": [
    "print(q_tokens)\n",
    "print(q_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens=q_tokens\n",
    "word_tokens=q_words\n",
    "a_tokens=[]\n",
    "a_words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    "\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you. Please contact the management\"\n",
    "        return robo_response\n",
    "    elif req_tfidf<0.5:\n",
    "        robo_response=robo_response+sent_tokens[idx]+\" This answer might be incorrect, please structure your query in a better way. If not, please contact management \"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]#remember senttoken[idx] is the token cosine similaritywise matched and retrieved\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. Greet me with a 'hi' to begin. If you want to exit, type 'bye'.You can then also access Management details.\n",
      "hi\n",
      "ROBO: hi\n",
      "when gravitas\n",
      "ROBO: I am sorry! I don't understand you. Please contact the management\n",
      "I am sorry! I don't understand you. Please contact the management\n",
      "I am sorry! I don't understand you. Please contact the management\n",
      "when riviera\n",
      "ROBO: I am sorry! I don't understand you. Please contact the management\n",
      "I am sorry! I don't understand you. Please contact the management\n",
      "I am sorry! I don't understand you. Please contact the management\n",
      "w\n",
      "ROBO: I am sorry! I don't understand you. Please contact the management\n",
      "I am sorry! I don't understand you. Please contact the management\n",
      "I am sorry! I don't understand you. Please contact the management\n",
      "w w w w w w\n",
      "ROBO: I am sorry! I don't understand you. Please contact the management\n",
      "I am sorry! I don't understand you. Please contact the management\n",
      "I am sorry! I don't understand you. Please contact the management\n",
      "bye\n",
      "ROBO: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "lenaw=0\n",
    "searchkey=0\n",
    "a_box=''\n",
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. Greet me with a 'hi' to begin. If you want to exit, type 'bye'.You can then also access Management details.\")\n",
    "while(flag==True):\n",
    "    user_response = input('')\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                \n",
    "                #entityextract(user_response)\n",
    "                a=response(user_response)\n",
    "                \n",
    "                sent_tokens.remove(user_response)\n",
    "                sent_tokens=[]\n",
    "                word_tokens=[]\n",
    "                p=len(l)            \n",
    "                for i in range (0,p):\n",
    "                    if a == l[i][0]:       \n",
    "                        searchkey=l[i][1]\n",
    "                q=len(o)            \n",
    "                for i in range (0,q):\n",
    "                    if o[i][1] == searchkey:       \n",
    "                        #                obtained all tokens append\n",
    "                        d=o[i][0]\n",
    "                        #print(d)\n",
    "                        d=d.lower()\n",
    "                        a_tokens.append(d)\n",
    "                        a_words=nltk.word_tokenize(d)\n",
    "                        #print(a_tokens)\n",
    "                        #print(a_words)\n",
    "                        #print('hiii')\n",
    "                        lenaw=len(a_words)\n",
    "                        for i in range(0,lenaw):\n",
    "                            if a_words[i]=='.':\n",
    "                                del a_words[i]\n",
    "                        a_words=a_words#+ee #a_entity should simply add tokens, should probably need no processing\n",
    "                        #make a_words unique\n",
    "                        a_words=unique(a_words)\n",
    "                        a_box=\" \".join(a_words)\n",
    "                        #\n",
    "                        \n",
    "                        #set up corpus from wiki\n",
    "                        f=open('rivierawiki.txt','r',errors = 'ignore')\n",
    "                        raw=f.read()\n",
    "                        raw=raw.lower()\n",
    "                        sent_tokens = nltk.sent_tokenize(raw)\n",
    "                        word_tokens = nltk.word_tokenize(raw)\n",
    "                        #\n",
    "                        b=response(a_box)\n",
    "                        print(b)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
